{"meta":{"title":"cyl's blog","subtitle":"","description":"","author":"yl.chen","url":"https://ysln.github.io","root":"/"},"pages":[{"title":"categories","date":"2021-05-01T09:11:22.000Z","updated":"2021-05-01T10:46:03.183Z","comments":true,"path":"categories/index.html","permalink":"https://ysln.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-05-01T10:46:40.000Z","updated":"2021-05-01T10:47:09.631Z","comments":true,"path":"tags/index.html","permalink":"https://ysln.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"spinlock & mutex & futex","slug":"spin-mutex-futex","date":"2021-05-05T02:25:50.018Z","updated":"2021-05-05T02:14:07.066Z","comments":true,"path":"2021/05/05/spin-mutex-futex/","link":"","permalink":"https://ysln.github.io/2021/05/05/spin-mutex-futex/","excerpt":"","text":"参考资料: 2021 南京大学 “操作系统：设计与实现” (蒋炎岩) JEFF DEAN的STANFORD演讲 前言在 CoolShell 上看到一篇文章, Jeff Dean 2010年在斯坦福大学的一场题为”Building Software Systems at Google and Lessons Learned”演讲中, 提到的 Numbers Everyone Should Know : 上图中的数据是2010年左右的, 放到现在来说可能不是太精确(而且对不同硬件配置来说也不一样). 但是这些数据本身也不是为了精确的衡量各种延迟, 主要是看一下数据的数量级, 以及不同项之间的纵向的比率. Teach Yourself Programming in Ten Years 这篇文章里也提到了这些数据. 上图中的数据最令我震惊的是Mutex lock/unlock居然只要25ns ! 但我在许多地方都看到过这样的言论: “加锁操作的开销是昂贵的, 加锁可能导致性能下降”. 所以我一直以为lock/unlock操作开销至少也得几百微秒, 甚至毫秒级. 但上图颠覆了我的认知, 于是, 亲自做个实验验证一下. mutex的开销测试的代码如下, 只是单纯的在单线程程序中进行N次lock/unlock操作, 统计花费的时间: 123456789101112131415161718192021222324252627282930#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;time.h&gt;#include &lt;pthread.h&gt;#define N 100000pthread_mutex_t mutex;int main() &#123; pthread_mutex_init(&amp;mutex, NULL); struct timespec start; clock_gettime(CLOCK_REALTIME, &amp;start); for (int i = 0; i &lt; N; ++i) &#123; pthread_mutex_lock(&amp;mutex); pthread_mutex_unlock(&amp;mutex); &#125; struct timespec end; clock_gettime(CLOCK_REALTIME, &amp;end); long time = (end.tv_sec - 1 - start.tv_sec) * 1000000000 + (end.tv_nsec + 1000000000 - start.tv_nsec); printf(&quot;Test lock/unlock for [%d] times.\\n&quot; &quot;Total takes [%ld] ns.\\n&quot; &quot;One cycle takes [%ld] ns.\\n&quot;, N, time, time / N); return 0;&#125; 使用gcc编译(不优化), 测试的机器的处理器为 “Intel(R) Core(TM) i5-6300HQ CPU @ 2.30GHz 2.30 GHz“, 系统为VirtualBox虚拟机上运行的Ubuntu 18.04, 虚拟机配置为两个CPU. 程序的运行结果如下: 123Test lock&#x2F;unlock for [100000] times.Total takes [3223049] ns.One cycle takes [32] ns. 结果和Jeff Dean提到的数据确实也基本吻合, 几十纳秒的数量级! fast path &amp; slow path按理来说, 实验到这里也就结束了, 但是之后在看jyy老师的OS课程时, jyy提到操作系统的两种锁实现: spinlock和mutexlock(姑且叫这个名字). spinlock只要使用一条原子指令就可以完成, x86平台下提供的lock前缀的指令就可以达到这个目的, 比如lock xchg: 1234567891011121314151617181920// equal to ==&gt; swap(*addr, newval)intptr_t atomic_xchg(volatile intptr_t *addr, intptr_t newval) &#123; intptr_t result; asm volatile (&quot;lock xchg %0, %1&quot; : &quot;+m&quot;(*addr), &quot;=a&quot;(result) : &quot;1&quot;(newval) : &quot;cc&quot;); return result;&#125;void lock(lock_t *lock) &#123; while (atomic_xchg(&amp;lock-&gt;locked, 1)) &#123; // wait &#125;&#125;void unlock(lock_t *lock) &#123; // wake atomic_xchg(&amp;locked-&gt;locked, 0);&#125; 因此, spinlock不需要经过系统调用就可以实现线程间的互斥访问共享资源, 众所周知系统调用的开销相对来说是很大的(一般为微秒级). 但是, 在spinlock上的等待的线程不断地执行lock xchg, 白白浪费宝贵的处理器时间. 一种合理的改进方法是, 当线程获取锁失败时, 不是让它满等待, 而是让操作系统把线程的执行状态改成某个标识, 比如blocking in a lock, 下次操作系统调度线程执行时就不再调度它执行, 直到等待的锁被释放, 才把该线程的状态改成runnable. 姑且把这种锁机制称为mutexlock. 从上述的描述中可以发现, mutexlock需要操作系统的支持, 也就是说操作系统需要提供一个类似SYS_lock的系统调用接口, 而系统调用的开销较大, 如果POSIX mutex使用这种实现, 我相信lock/unlock的时间肯定不止几十纳秒. spinlock和mutexlock各有其优劣, 那么能不能结合两种锁各自的优点, 设计一种新的锁机制? 当然可以! 这背后的思想便是jyy多次提到的 fast path &amp; slow path (jyy在讲内存分配的设计时也提到了这种思想). fast path: 只需要一条原子指令(比如lock xchg), 上锁成功立即返回. slow path: 上锁失败, 调用系统调用, 让操作系统调度其他线程执行(相当于进入睡眠). 这便是 Futex(Fast Userspace muTexes) 的设计思想. 但是使用上面这种方式, unlock时至少也需要一次系统调用, 用来”唤醒”在该锁上blocking的线程, 而如果此时没有其他线程在等待这把锁, 此次系统调用就是多余的. 然而实际使用的Futex的实现中不会有这个问题, 它使用了一些巧妙的设计避免了unlock时不必要的系统调用. 深入验证futex是Linux系统提供的系统调用, POSIX mutex就是用futex机制实现的. 下面深入验证一下上文的验证程序是否调用了该系统调用. strace使用strace来跟踪程序使用的系统调用, 过滤一些输出: 可以看到没有调用futex. 会不会是被编译器优化了? 使用objdump看一下编译生成的 a.out: 可以看到确实有调用pthread_mutex_lock/pthread_mutex_unlock. 那么看一下一个典型的多线程程序(多个线程并发对全局变量执行自增操作, 用mutex保护该全局变量)的结果: 1234567891011121314151617181920212223242526272829#include &quot;threads.h&quot;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;time.h&gt;#include &lt;pthread.h&gt;#define N 1000000volatile long sum = 0;pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;void do_sum(int unused) &#123; for (int i = 0; i &lt; N; ++i) &#123; pthread_mutex_lock(&amp;mutex); sum++; pthread_mutex_unlock(&amp;mutex); &#125;&#125;int main() &#123; for (int i = 0; i &lt; 10; ++i) &#123; create(do_sum); &#125; join_all(); printf(&quot;sum = %ld\\n&quot;, sum); return 0;&#125; 这里使用了jyy提供的”threads.h”, 其实就是封装了一下pthread库, 用起来更简单些. strace的结果如下(注意需要使用-f选项): 可以看到确实调用了futex, 并且每次调用的平均耗时为几十微秒: GDB原始测试程序(单线程)接下来使用gdb来调试一下原始那个测试程序. 注意这个程序是单线程程序. 单步调试进入pthread_mutex_lock函数. lock cmpxchg便是spinlock中提到的原子交换指令. 单步运行si: 执行原子交换指令后, 下一条je指令判断成功, 跳转到目标处执行, 然后函数返回retq. 这种情况对于的是pthread_mutex_lock函数获取锁成功, 立即返回, 可以看到没有执行任何系统调用(syscall). 那如果获取失败呢? 可以从上图中看到je后面的第3条指令调用了__lll_lock_wait函数, 这个先不管它, 之后会调试. 单步调试进入pthread_mutex_unlock函数: 可以看到这里的原子指令是 lock decl(相当于原子的自减操作), 而不是lock cmpxchg. 单步运行si: 执行原子交换指令后, 下一条je指令判断成功, 跳转到目标处执行, 然后函数返回retq. 这种情况对于的是pthread_mutex_unlock函数解锁, 并且没有其他线程在等待这把锁, 立即返回, 可以看到没有执行任何系统调用(syscall). 如果je判断失败呢? 这对应的其实是释放锁时, 有其他线程在等待这把锁, 所以要调用__lll_unlock_wake函数, 这里先不管它, 之后会调试. 典型多线程程序现在调试一下典型多线程程序(即上面第2个程序). 为了方便调试, 只创建2个子线程(包括主线程, 总共有3个线程). 在do_sum函数打断点, 运行, 然后set scheduler-locking on, 这个命令的作用是: 调试当前线程时, 让其他线程处于停止的状态. 当前是 thread 2在运行, LWP 26992是该线程的ID, 之后会用到该ID. 先调试该线程. 进入pthread_mutex_unlock函数, 可以预料到它可以成功执行pthread_mutex_lock, 获取到全局变量mutex的所有权, 结果也确实如此: mutex的Owner ID确实是 thread 2的ID, mutex的状态是**Acquired, possibly with no waiters**, 表示已经被上锁, 并且没有其他线程在等待该锁. 现在切换到 thread 3运行. 进入pthread_mutex_lock函数, 由于全局的锁mutex已经被 thread 2获取了, 可以预料到 该函数调用将会失败 原子交换指令将会发现”失败”: 结果确实如此, je判断失败, 然后调用__lll_lock_wait函数. 进入__lll_lock_wait函数: 可以看到确实执行了syscall指令, 0xca/202是系统调用编号: 确实是 futex! 这时 thread 3阻塞在获取mutex, 相当于进入了”睡眠”. 切换回 thread 2执行: 此时mutex的状态是 Acquired, possibly with waiters, 表示有其他线程在等待获取该锁. 进入pthread_mutex_lock函数, 由于 thread 3此时正在等待这把锁, 可以预料到lock decl之后的判断将会失败: 可以看到确实是这样. 进入__lll_unlock_wake函数: 可以看到确实调用了0xca号系统调用, 即 futex! 切换回 thread 3, 由于 thread 2释放了mutex, 可以预料到 thread 3从syscall返回后会成功获取到mutex的所有权: mutex的Owner ID确实是 thread 3. 不过mutex的status有点诡异, 因为按理来说此时应该没有其他线程在等待该锁. Futex自行查阅Linux man pages: man futex. 后语在最近的工作中, 使用了libusb库. 程序使用的是库推荐的Asynchronous I/O, 代码模式使用的也是文档的示例模式: 创建一个单独的线程, 不停的调用libusb_handle_events处理异步的事件. 查阅文档, 看到libusb_handle_events可以在多个线程中安全的使用, 因为它内部有一个event_lock. 但是我使用的程序中只用了一个线程调用libusb_handle_events来处理异步事件, 于是换成libusb_handle_events_locked接口, 省去额外的lock/unlock. 但是, 实际结果, 两种方式差别不大. 通过上文的分析, 该结果也不难理解. 此外, 在看c++的std::shared_ptr时, 我看到有的地方说shared_ptr内部的引用计数为了thread-safety, 有相应的锁保护, 如果过度使用它, 可能会有性能问题. 真的是这样吗? 有多大的性能下降?","categories":[{"name":"NJU OS Lab","slug":"NJU-OS-Lab","permalink":"https://ysln.github.io/categories/NJU-OS-Lab/"}],"tags":[{"name":"C","slug":"C","permalink":"https://ysln.github.io/tags/C/"},{"name":"thread","slug":"thread","permalink":"https://ysln.github.io/tags/thread/"},{"name":"mutex&futex","slug":"mutex-futex","permalink":"https://ysln.github.io/tags/mutex-futex/"},{"name":"gdb","slug":"gdb","permalink":"https://ysln.github.io/tags/gdb/"},{"name":"调试","slug":"调试","permalink":"https://ysln.github.io/tags/%E8%B0%83%E8%AF%95/"}]},{"title":"硬件线程(hart)&软件线程&CPU","slug":"threads-on-multi-CPU","date":"2021-05-05T01:02:36.051Z","updated":"2021-05-05T02:14:56.147Z","comments":true,"path":"2021/05/05/threads-on-multi-CPU/","link":"","permalink":"https://ysln.github.io/2021/05/05/threads-on-multi-CPU/","excerpt":"","text":"参考资料: L2: 多处理器内核上的线程管理 (kmt) 本文是在完成NJU操作系统实验过程中的一些记录. 背景“多处理器内核上的线程管理 (kmt)”这个实验并不是很难, 前提是认真听了jyy的课, 以及仔细阅读了jyy的实验指南. 由于没有认真听课, 所以做实验的时候踩了很多坑, 各种各样的并发bug, 虚拟机各种神秘重启. 这个实验和NJU”计算机系统基础”实验PA3&amp;PA4有点像有点关联, 但两者侧重点不同. PA3&amp;PA4的侧重点是发生中断/异常时context的保存与恢复, 以及怎样创建内核线程(包括context的创建与放置, 如何跳转执行). 而且实验模拟的硬件是单CPU的. 这个实验中context的store/restore已经实现好了, 侧重点是多处理器内核上的线程管理, 关键词: 多CPU, 线程调度. 实现错误的尝试一开始, 我的做法是把所有的tasks(相当于线程)组织成一个链表, 用一把大锁(spinlock)保护这个链表. 然后调度发生时, 所有CPU都从这个链表取出可以运行的task执行. 按理来说这是最简单粗暴(但不好)的方法, 应该可以工作. 就像实现内存分配器一样, 先实现用一把大锁保护资源的方案, 然后去掉大锁, 换成CPU-local的方案. 当只创建一些执行简单代码的内核线程时(比如打印字符串), 看起来可以正常工作(好像偶尔还是会panic). 直到实现semaphore, 并创建生产者-消费者线程, 内核开始各种panic, 虚拟机各种神秘重启. 而且大多数情况下都是保护tasks链表的spinlock发生了错误: 同一个CPU重复对该spinlock上锁. semaphore不同于spinlock, 当一个task获取spinlock失败时会一直”自旋”等待, 并不会让出CPU(关闭了中断). 但是当一个task对semaphore执行P操作失败时, 会把当前线程的状态置为SLEEPING, 表示当前task不能被调度执行, 然后执行yield, 主动让出CPU. 直到其他task对该semaphore执行V操作, 唤醒一个等待该semaphore的task. 对semaphore执行V操作可以发生在任意地方, 包括中断处理程序里. 问题就出现在所有CPU都对同一个tasks链表进行操作. (我到现在也不知道到底是不是这个问题导致的). 中断发生时(比如时钟中断), 就是线程调度的时刻. 而每个CPU都是独立响应中断的, 所以线程调度是并发的. 我用尽了各种方法来调试这个问题, 包括gdb, 还是无法找出问题的关键. 当我被折磨得生不如死的时候, 我重新去看了jyy的课, 我确定了新的方案. 有效方案把所有的tasks组织成一个链表看起来很简单, 但是有一个问题: 一个task有可能会被调度到任意一个CPU上去执行. 为什么不做的更简单彻底一点, 即: 把task绑定到固定的CPU上. 因此新的方案是: tasks链表实现成CPU-local的, 每个CPU只从自己的tasks链表中取出task来执行. 这样在调度发生时, CPU之间不需要争抢同一个spinlock, 效率更高. 即使这样, 每个CPU的tasks链表还是分别需要独立的spinlock保护起来. 关于CPU各自的tasks链表是否需要spinlock保护起来, 需要仔细考虑. 目前的使用场景下, 不需要, 因为在多处理器环境开启之前, 就由CPU0创建好了所有的tasks, 并把tasks分配到每个CPU上. 而且所有的task都不会返回, task不需要回收. 也就是说, 没有链表的插入节点与删除节点的操作. 当然, 使用spinlock也没有问题. 之后拓展使用场景时可能就需要spinlock, 比如: 运行时创建task, 销毁回收task, CPU之间的task迁移(负载均衡?). 坑下图截取自jyy的实验指南. jyy的代码实现很巧妙, 也很优雅. 但是有一点”误导”了我(我自己的锅, 理解有误). os-&gt;on_irq是用来注册中断处理函数的, 当中断/异常发生时, 便会根据发生的事件, 按序调用注册的handler function. kmt_context_save这个函数的名字让我产生了误解. save的是context这个结构体呢? 还是context指针? 其实这里的语义是把已经保存在当前task的内核栈上的context的地址/指针, 保存在该task相关的数据结构里面. 这里kemt_context_save和kmt_schedule分开来了, 并且 kemt_context_save总是最先调用, kmt_schedule总是最后调用. 之前做”计算机系统基础”实验PA3时, 我把两部分都放在了调度函数(类似于kmt_schedule)里面. 按照正常的逻辑来说, 两种做法其实是等价的. 那什么是非正常的逻辑? 在handler function(比如时钟/键盘中断处理函数)里面调用yield. 为了方便描述, 假设: 方案A: kemt_context_save和kmt_schedule分开, 两者之间可以调用任意的handler function. 方案B: kemt_context_save合并进kmt_schedule里面, 相当于两者是原子的, 所有的handler function调用完毕才会调用kmt_schedule. 哪一种方案在这种特殊/非正常情景下有问题? schedule的功能大致如下: 1234567891011Context *schedule(Event ev, Context *ctx) &#123; // 方案B等价于在这里调用yield current_task-&gt;context = ctx; // (1) 保存context指针 // 方案A等价于在这里调用yield current_task = select_next_task(); // (2) 选择一个可运行的task return current_task-&gt;context; // 之后的代码会把选中的task的context恢复到当前CPU上&#125; 方案A等价于在上述代码第6行中调用了yield, 此时又会引发新的一轮context的store/restore: 当yield返回时(某次调度发生时, 选择了该task运行, 此时表明ctx 2已经从该task的内核栈上销毁), curr-&gt;ctx并没有得到更新: 应该指向ctx 1, 但还是指向yield发生时, 保存到栈上的context(即ctx 2), 但到当前这个时刻ctx 2指向的区域是无效的. 而方案B没有以上问题, 因为传递给schedule函数的ctx参数是保存在该task的内核栈上的(即上图中的stack variable区域, 每个保存在栈上的context后面都会有), 栈上变量在函数的作用域内是有效的, 这是由C语言的机制保证的, ctx参数总是指向最近一次保存在内核栈上的context的地址. 延申思考idle task之前在看Linux内核相关的资料时, 好像在哪看到了idle task这个说法, 一直没有理解这是个啥. 做完这个实验, 我好像有点懂了. 以下是个人理解. 以前玩单片机时, 程序里先是注册各种中断处理程序, 在最后面总是要加上 while (1);, 然后就开始响应各种中断. 这里的 while (1);其实就可以理解成idle task. CPU总是要干点什么, 即使只是死循环, 不然它就直接shutdown了. 操作系统代码也是一样, 在把所有资源初始化好之后, 它就退变为一个中断响应程序. 因此, 它也需要一个idle task, 以防止CPU shutdown. 当没有可运行的task可以调度执行时, 总是需要调度idle task到CPU上执行. 中断处理函数里不能休眠之前在看”Linux内核设计与实现”这本书时, 里面提到不能在中断上下文中调用会引起睡眠/阻塞的函数: 中断上下文和进程并没有什么瓜葛. 与current宏也是不相干的(尽管它会指向被中断的进程). 因为没有后备进程, 所以中断上下文不可以睡眠, 否则怎能再对它重新调度呢? 我不太清楚Linux内核关于context的store/restore, 所以对这句话不是很理解. 根据我做完实验的理解, 睡眠即相当于主动调用yield, 让出CPU的执行权. 上文也分析了, 在当前的代码实现中, 在中断处理程序(handler function)里面调用yield, 根据实验现象来看, 无论是单CPU还是多CPU都是没有什么问题的. 还需要仔细思考这个问题. 操作系统是一个并发程序jyy在上课时总是在说这句话, 而且在课程安排上, 花了很多时间来讲并发, 和一些看起来和写操作系统代码无关的东西. 做完这个实验, 我又懂了. 当使用比如POSIX threadsAPI创建线程时, 我们知道线程创建之后就会开始并发执行, 并且所有线程共享同一地址空间的全部内容, 比如全局/静态的数据. 每个线程都可以访问并修改全局变量, 并且修改后对其他线程也是可见的. 因此, 我们知道对于对于共享的数据, 需要使用锁保护起来, 比如mutex来提供互斥. 把视角切换回操作系统内核. CPU可以想象成上面提到的”线程”, 而操作系统内核代码可以想象成我们写的多线程程序. 因此, 所有的CPU执行的都是同一份代码, 代码里面所有的全局/静态数据, CPU都可以访问/修改, 并且修改后CPU之间也是可见的. 所以操作系统的代码里同样需要锁来保护共享的数据. 比如xv6的这段代码: CPU编号为0的CPU会执行if分支的代码, 其他所有的CPU执行else分支的代码. 由于内核里所有的全局数据都是共享的(比如文件系统相关的资源, 进程表等), 所以只需要由CPU0来初始化一次(当然可以由任意一个CPU来完成这个工作). 如果你足够细心, 你会发现上图中else分支里调用的函数都有hart后缀. 另外, 这里的scheduler也起到了前面提到的idle task的作用. 硬件线程(hart)通过阅读RISC-V的手册, 我得到了以下信息: hart是硬件线程(hardware thread)的缩略形式. 我们用该术语将它们与大多数程序员熟悉的软件线程区分开来. 软件线程在harts上进行分时复用. 大多数处理器核都只有一个hart. 简单理解, 一个CPU (我不太理解CPU与CPU core的区别, 这里把它们都称作CPU) 就是一个硬件线程(hart), 用户/内核创建的一个软件线程, 对应的就是CPU-local的tasks链表中的一个task. 此外, 我又想到了AMD的县城线程撕裂者:","categories":[{"name":"NJU OS Lab","slug":"NJU-OS-Lab","permalink":"https://ysln.github.io/categories/NJU-OS-Lab/"}],"tags":[{"name":"C","slug":"C","permalink":"https://ysln.github.io/tags/C/"},{"name":"OS","slug":"OS","permalink":"https://ysln.github.io/tags/OS/"},{"name":"thread","slug":"thread","permalink":"https://ysln.github.io/tags/thread/"},{"name":"hart","slug":"hart","permalink":"https://ysln.github.io/tags/hart/"}]},{"title":"C语言实现Real-Eval-Print-Loop (REPL)","slug":"crepl","date":"2021-05-01T07:11:46.598Z","updated":"2021-05-05T01:13:15.097Z","comments":true,"path":"2021/05/01/crepl/","link":"","permalink":"https://ysln.github.io/2021/05/01/crepl/","excerpt":"","text":"参考资料: M4: C Real-Eval-Print-Loop (crepl) 本文是在完成NJU操作系统实验过程中的一些记录. 很多现代编程语言都提供了交互式的REPL, 比如Python这种”解释执行”的语言. 在REPL中, 除了进行简单的数值计算外(类似计算器), 还可以动态创建函数, 并在之后调用它. 这个实验的目的是使用C语言实现一个简易的REPL. 最终效果如下: 这个技术和现代虚拟机中的即时编译 (just-in-time) 技术是非常相关的：在程序运行时 (而非程序执行前) 进行编译，并将编译得到的二进制代码 (指令/数据) 动态加载。 — 摘自jyy的实验指南. 分析C语言是一种编译型的语言, 源文件到二进制文件需要经过编译和链接(link)两个过程. 链接主要完成的工作是符号解析和重定位. 如果是静态链接的程序(-static), 类型为”Executable file(EXEC)”, 即可执行文件. 程序中的所有符号都是被完全解析的, 比如所有函数符号的地址都是确定的, 可以直接调用. GCC默认生成的二进制文件是动态链接的, 生成的文件文件类型为”Shared object file(DYN)”. 一个动态链接的程序从磁盘(disk)上刚加载出来时是 不完整(部分链接) 的, 比如程序调用的位于glibc动态链接库中的printf函数地址是未知的. 内核(加载器)不能立即把控制权交给该程序(即跳转到entry). 因此, 需要另外的辅助程序(helper program)的帮助 ——– dynamic linker, (一般是ld.so). 它的任务是: 加载程序依赖的DSOs(动态链接库, 比如glibc), 并完成重定位(relocation). 因此, 要想实现一个交互式的REPL, 在程序运行时动态的在其地址空间内加入新的函数符号, 本质上是实现动态链接器的功能. 实现C语言实现的REPL的工作流程如下: 读取用户的输入. 把输入的内容写入一个临时的源文件, 并把其编译成一个动态链接库. 把动态链接库加载到进程的地址空间. 计算用户输入的表达式, 输出结果. 重复上述步骤. 编译 这里有以下注意事项: 如果用户输入不是函数, 而是表达式, 比如”5 + 6”, 该怎么办? 一个很自然(看完jyy的实验指南后确实很自然…)的想法是把表达式”包装(wrap)”成一个函数, 比如: 1int __wrap_func() &#123; return 5 + 6;&#125; 这里实现”即时编译”的方法是fork一个子进程, 让子进程执行GCC, 主要是exec函数族(l, v, p, e的组合)的使用, 要注意argv[0]需要为可执行文件的名字. 注意这里把子进程的标准输出和标错误都关闭了, 防止gcc的输出影响. 这里还有一点是如果用户输入”f() + 6”, 那么源文件的内容为: 1int __wrap_func() &#123; return f() + 6;&#125; 此时gcc编译会给出警告: “f”未定义. 如果”f”在之前定义了, 那么这个警告就无关紧要. 就算”f”没有定义, 也没有关系, 可以把出错处理推迟到加载阶段. 加载 可以使用dlopen在运行时加载一个so文件到当前进程的地址空间中. 这里需要注意dlopen的flag参数: RTLD_GLOBAL是必不可少的, 其含义是: 当前加载的so中的符号对其他so可见, 也就是说之后加载的so符号解析时, 可以使用这个so中的符号. 与其相对的是RTLD_LOCAL. 如果去掉RTLD_GLOBAL, 会出现以下这种情况: 注意表达式”f()”也会被”wrap”成一个函数, 并编译成so. RTLD_NOW也是必不可少的, 其含义是: 在加载so时就进行(函数)符号解析. 与之相对的是RTLD_LAZY, 表示对应的函数符号解析推迟到函数被调用时. 使用RTLD_LAZY参数有以下实验现象: 也就是说, 定义函数时可以使用当前还未定义的符号, dlopen时不会报错(因为不会进行符号解析). 但是调用时如果找不到对应的函数符号, 进程会直接crash, 结束运行. 而使用RTLD_NOW参数有以下实验现象: 也就是说定义函数时不允许使用当前还未定义的符号, 也不能直接使用未定义的函数计算, dlopen进行符号解析时就会出错. 因此, 这才是我们想要的行为.","categories":[{"name":"NJU OS Lab","slug":"NJU-OS-Lab","permalink":"https://ysln.github.io/categories/NJU-OS-Lab/"}],"tags":[{"name":"C","slug":"C","permalink":"https://ysln.github.io/tags/C/"},{"name":"REPL","slug":"REPL","permalink":"https://ysln.github.io/tags/REPL/"},{"name":"动态链接","slug":"动态链接","permalink":"https://ysln.github.io/tags/%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5/"}]}],"categories":[{"name":"NJU OS Lab","slug":"NJU-OS-Lab","permalink":"https://ysln.github.io/categories/NJU-OS-Lab/"}],"tags":[{"name":"C","slug":"C","permalink":"https://ysln.github.io/tags/C/"},{"name":"thread","slug":"thread","permalink":"https://ysln.github.io/tags/thread/"},{"name":"mutex&futex","slug":"mutex-futex","permalink":"https://ysln.github.io/tags/mutex-futex/"},{"name":"gdb","slug":"gdb","permalink":"https://ysln.github.io/tags/gdb/"},{"name":"调试","slug":"调试","permalink":"https://ysln.github.io/tags/%E8%B0%83%E8%AF%95/"},{"name":"OS","slug":"OS","permalink":"https://ysln.github.io/tags/OS/"},{"name":"hart","slug":"hart","permalink":"https://ysln.github.io/tags/hart/"},{"name":"REPL","slug":"REPL","permalink":"https://ysln.github.io/tags/REPL/"},{"name":"动态链接","slug":"动态链接","permalink":"https://ysln.github.io/tags/%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5/"}]}