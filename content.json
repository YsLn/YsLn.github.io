{"meta":{"title":"cyl's blog","subtitle":"","description":"","author":"yl.chen","url":"https://ysln.github.io","root":"/"},"pages":[{"title":"categories","date":"2021-05-01T09:11:22.000Z","updated":"2021-05-01T10:46:03.183Z","comments":true,"path":"categories/index.html","permalink":"https://ysln.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-05-01T10:46:40.000Z","updated":"2021-05-01T10:47:09.631Z","comments":true,"path":"tags/index.html","permalink":"https://ysln.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"硬件线程(hart)&软件线程&CPU","slug":"threads-on-multi-CPU","date":"2021-05-04T04:58:49.186Z","updated":"2021-05-04T14:36:48.685Z","comments":true,"path":"2021/05/04/threads-on-multi-CPU/","link":"","permalink":"https://ysln.github.io/2021/05/04/threads-on-multi-CPU/","excerpt":"","text":"参考资料: L2: 多处理器内核上的线程管理 (kmt) 本文是在完成NJU操作系统实验过程中的一些记录. 背景“多处理器内核上的线程管理 (kmt)”这个实验并不是很难, 前提是认真听了jyy的课, 以及仔细阅读了jyy的实验指南. 由于没有认真听课, 所以做实验的时候踩了很多坑, 各种各样的并发bug, 虚拟机各种神秘重启. 这个实验和NJU”计算机系统基础”实验PA3&amp;PA4有点像有点关联, 但两者侧重点不同. PA3&amp;PA4的侧重点是发生中断/异常时context的保存与恢复, 以及怎样创建内核线程(包括context的创建与放置, 如何跳转执行). 而且实验模拟的硬件是单CPU的. 这个实验中context的store/restore已经实现好了, 侧重点是多处理器内核上的线程管理, 关键词: 多CPU, 线程调度. 实现错误的尝试一开始, 我的做法是把所有的tasks(相当于线程)组织成一个链表, 用一把大锁(spinlock)保护这个链表. 然后调度发生时, 所有CPU都从这个链表取出可以运行的task执行. 按理来说这是最简单粗暴(但不好)的方法, 应该可以工作. 就像实现内存分配器一样, 先实现用一把大锁保护资源的方案, 然后去掉大锁, 换成CPU-local的方案. 当只创建一些执行简单代码的内核线程时(比如打印字符串), 看起来可以正常工作(好像偶尔还是会panic). 直到实现semaphore, 并创建生产者-消费者线程, 内核开始各种panic, 虚拟机各种神秘重启. 而且大多数情况下都是保护tasks链表的spinlock发生了错误: 同一个CPU重复对该spinlock上锁. semaphore不同于spinlock, 当一个task获取spinlock失败时会一直”自旋”等待, 并不会让出CPU(关闭了中断). 但是当一个task对semaphore执行P操作失败时, 会把当前线程的状态置为SLEEPING, 表示当前task不能被调度执行, 然后执行yield, 主动让出CPU. 直到其他task对该semaphore执行V操作, 唤醒一个等待该semaphore的task. 对semaphore执行V操作可以发生在任意地方, 包括中断处理程序里. 问题就出现在所有CPU都对同一个tasks链表进行操作. (我到现在也不知道到底是不是这个问题导致的). 中断发生时(比如时钟中断), 就是线程调度的时刻. 而每个CPU都是独立响应中断的, 所以线程调度是并发的. 我用尽了各种方法来调试这个问题, 包括gdb, 还是无法找出问题的关键. 当我被折磨得生不如死的时候, 我重新去看了jyy的课, 我确定了新的方案. 有效方案把所有的tasks组织成一个链表看起来很简单, 但是有一个问题: 一个task有可能会被调度到任意一个CPU上去执行. 为什么不做的更简单彻底一点, 即: 把task绑定到固定的CPU上. 因此新的方案是: tasks链表实现成CPU-local的, 每个CPU只从自己的tasks链表中取出task来执行. 这样在调度发生时, CPU之间不需要争抢同一个spinlock, 效率更高. 即使这样, 每个CPU的tasks链表还是分别需要独立的spinlock保护起来. 关于CPU各自的tasks链表是否需要spinlock保护起来, 需要仔细考虑. 目前的使用场景下, 不需要, 因为在多处理器环境开启之前, 就由CPU0创建好了所有的tasks, 并把tasks分配到每个CPU上. 而且所有的task都不会返回, task不需要回收. 也就是说, 没有链表的插入节点与删除节点的操作. 当然, 使用spinlock也没有问题. 之后拓展使用场景时可能就需要spinlock, 比如: 运行时创建task, 销毁回收task, CPU之间的task迁移(负载均衡?). 坑下图截取自jyy的实验指南. jyy的代码实现很巧妙, 也很优雅. 但是有一点”误导”了我(我自己的锅, 理解有误). os-&gt;on_irq是用来注册中断处理函数的, 当中断/异常发生时, 便会根据发生的事件, 按序调用注册的handler function. kmt_context_save这个函数的名字让我产生了误解. save的是context这个结构体呢? 还是context指针? 其实这里的语义是把已经保存在当前task的内核栈上的context的地址/指针, 保存在该task相关的数据结构里面. 这里kemt_context_save和kmt_schedule分开来了, 并且 kemt_context_save总是最先调用, kmt_schedule总是最后调用. 之前做”计算机系统基础”实验PA3时, 我把两部分都放在了调度函数(类似于kmt_schedule)里面. 按照正常的逻辑来说, 两种做法其实是等价的. 那什么是非正常的逻辑? 在handler function(比如时钟/键盘中断处理函数)里面调用yield. 为了方便描述, 假设: 方案A: kemt_context_save和kmt_schedule分开, 两者之间可以调用任意的handler function. 方案B: kemt_context_save合并进kmt_schedule里面, 相当于两者是原子的, 所有的handler function调用完毕才会调用kmt_schedule. 哪一种方案在这种特殊/非正常情景下有问题? schedule的功能大致如下: 1234567891011Context *schedule(Event ev, Context *ctx) &#123; // 方案B等价于在这里调用yield current_task-&gt;context = ctx; // (1) 保存context指针 // 方案A等价于在这里调用yield current_task = select_next_task(); // (2) 选择一个可运行的task return current_task-&gt;context; // 之后的代码会把选中的task的context恢复到当前CPU上&#125; 方案A等价于在上述代码第6行中调用了yield, 此时又会引发新的一轮context的store/restore: 当yield返回时(某次调度发生时, 选择了该task运行, 此时表明ctx 2已经从该task的内核栈上销毁), curr-&gt;ctx并没有得到更新: 应该指向ctx 1, 但还是指向yield发生时, 保存到栈上的context(即ctx 2), 但到当前这个时刻ctx 2指向的区域是无效的. 而方案B没有以上问题, 因为传递给schedule函数的ctx参数是保存在该task的内核栈上的, 栈上变量在函数的作用域内是有效的, 这是由C语言的机制保证的, ctx参数总是指向最近一次保存在内核栈上的context的地址. 延申思考idle task之前在看Linux内核相关的资料时, 好像在哪看到了idle task这个说法, 一直没有理解这是个啥. 做完这个实验, 我好像有点懂了. 以下是个人理解. 以前玩单片机时, 程序里先是注册各种中断处理程序, 在最后面总是要加上 while (1);, 然后就开始响应各种中断. 这里的 while (1);其实就可以理解成idle task. CPU总是要干点什么, 即使只是死循环, 不然它就直接shutdown了. 操作系统代码也是一样, 在把所有资源初始化好之后, 它就退变为一个中断响应程序. 因此, 它也需要一个idle task, 以防止CPU shutdown. 当没有可运行的task可以调度执行时, 总是需要调度idle task到CPU上执行. 中断处理函数里不能休眠之前在看”Linux内核设计与实现”这本书时, 里面提到不能在中断上下文中调用会引起睡眠/阻塞的函数: 中断上下文和进程并没有什么瓜葛. 与current宏也是不相干的(尽管它会指向被中断的进程). 因为没有后备进程, 所以中断上下文不可以睡眠, 否则怎能再对它重新调度呢?.我不太清楚Linux内核关于context的store/restore, 所以对这句话不是很理解. 根据我做完实验的理解, 睡眠即相当于主动调用yield, 让出CPU的执行权. 上文也分析了, 在当前的代码实现中, 在中断处理程序(handler function)里面调用yield, 根据实验现象来看, 无论是单CPU还是多CPU都是没有什么问题的. 还需要仔细思考这个问题. 操作系统是一个并发程序jyy在上课时总是在说这句话, 而且在课程安排上, 花了很多时间来讲并发, 和一些看起来和写操作系统代码无关的东西. 做完这个实验, 我又懂了. 当使用比如POSIX threadsAPI创建线程时, 我们知道线程创建之后就会开始并发执行, 并且所有线程共享同一地址空间的全部内容, 比如全局/静态的数据. 每个线程都可以访问并修改全局变量, 并且修改后对其他线程也是可见的. 因此, 我们知道对于对于共享的数据, 需要使用锁保护起来, 比如mutex来提供互斥. 把视角切换回操作系统内核. CPU可以想象成上面提到的”线程”, 而操作系统内核代码可以想象成我们写的多线程程序. 因此, 所有的CPU执行的都是同一份代码, 代码里面所有的全局/静态数据, CPU都可以访问/修改, 并且修改后CPU之间也是可见的. 所以操作系统的代码里同样需要锁来保护共享的数据. 比如xv6的这段代码: CPU编号为0的CPU会执行if分支的代码, 其他所有的CPU执行else分支的代码. 由于内核里所有的全局数据都是共享的(比如文件系统相关的资源, 进程表等), 所以只需要由CPU0来初始化一次(当然可以由任意一个CPU来完成这个工作). 如果你足够细心, 你会发现上图中else分支里调用的函数都有hart后缀. 另外, 这里的scheduler也起到了前面提到的idle task的作用. 硬件线程(hart)通过阅读RISC-V的手册, 我得到了以下信息: hart是硬件线程(hardware thread)的缩略形式. 我们用该术语将它们与大多数程序员熟悉的软件线程区分开来. 软件线程在harts上进行分时复用. 大多数处理器核都只有一个hart. 简单理解, 一个CPU*(我不太理解CPU与CPU core的区别, 这里把它们都称作CPU)*就是一个硬件线程(hart), 用户/内核创建的一个软件线程, 对应的就是CPU-local的tasks链表中的一个task. 此外, 我又想到了ADM的县城线程撕裂者:","categories":[{"name":"NJU OS Lab","slug":"NJU-OS-Lab","permalink":"https://ysln.github.io/categories/NJU-OS-Lab/"}],"tags":[{"name":"C","slug":"C","permalink":"https://ysln.github.io/tags/C/"},{"name":"OS","slug":"OS","permalink":"https://ysln.github.io/tags/OS/"},{"name":"thread","slug":"thread","permalink":"https://ysln.github.io/tags/thread/"},{"name":"hart","slug":"hart","permalink":"https://ysln.github.io/tags/hart/"}]},{"title":"C语言实现Real-Eval-Print-Loop (REPL)","slug":"crepl","date":"2021-05-01T07:11:46.598Z","updated":"2021-05-04T08:56:59.277Z","comments":true,"path":"2021/05/01/crepl/","link":"","permalink":"https://ysln.github.io/2021/05/01/crepl/","excerpt":"","text":"参考资料: M4: C Real-Eval-Print-Loop (crepl) 本文是在完成NJU操作系统实验过程中的一些记录. 很多现代编程语言都提供了交互式的REPL, 比如Python. 在REPL中, 除了进行简单的数值计算外(类似计算器), 还可以动态创建函数, 并在之后调用它. 基于”解释执行”的程序设计语言(包括shell本身)天生就具有这种交互式的工作模式. 这个实验的目的是使用C语言实现一个简易的REPL. 最终效果如下: 这个技术和现代虚拟机中的即时编译 (just-in-time) 技术是非常相关的：在程序运行时 (而非程序执行前) 进行编译，并将编译得到的二进制代码 (指令/数据) 动态加载。 — 摘自jyy的实验指南. 分析C语言是一种编译型的语言, 源文件到二进制文件需要经过编译和链接(link)两个过程. 链接主要完成的工作是符号解析和重定位. 如果是静态链接的程序(-static), 类型为”Executable file(EXEC)”, 即可执行文件. 程序中的所有符号都是被完全解析的, 比如所有函数符号的地址都是确定的, 可以直接调用. GCC默认生成的二进制文件是动态链接的, 生成的文件文件类型为”Shared object file(DYN)”. 一个动态链接的程序从磁盘(disk)上刚加载出来时是**不完整(部分链接)**的, 比如程序调用的位于glibc动态链接库中的printf函数地址是未知的. 内核(加载器)不能立即把控制权交给该程序(即跳转到entry). 因此, 需要另外的辅助程序(helper program)的帮助 ——– dynamic linker, (一般是ld.so). 它的任务是: 加载程序依赖的DSOs(动态链接库, 比如glibc), 并完成重定位(relocation). 因此, 要想实现一个交互式的REPL, 在程序运行时动态的在其地址空间内加入新的函数符号, 本质上是实现动态链接器的功能. 实现C语言实现的REPL的工作流程如下: 读取用户的输入. 把输入的内容写入一个临时的源文件, 并把其编译成一个动态链接库. 把动态链接库加载到进程的地址空间. 计算用户输入的表达式, 输出结果. 重复上述步骤. 编译 这里有以下注意事项: 如果用户输入不是函数, 而是表达式, 比如”5 + 6”, 该怎么办? 一个很自然(看完jyy的实验指南后确实很自然…)的想法是把表达式”包装(wrap)”成一个函数, 比如: 1int __wrap_func() &#123; return 5 + 6;&#125; 这里实现”即时编译”的方法是fork一个子进程, 让子进程执行GCC, 主要是exec函数族(l, v, p, e的组合)的使用, 要注意argv[0]需要为可执行文件的名字. 注意这里把子进程的标准输出和标错误都关闭了, 防止gcc的输出影响. 这里还有一点是如果用户输入”f() + 6”, 那么源文件的内容为: 1int __wrap_func() &#123; return f() + 6;&#125; 此时gcc编译会给出警告: “f”未定义. 如果”f”在之前定义了, 那么这个警告就无关紧要. 就算”f”没有定义, 也没有关系, 可以把出错处理推迟到加载阶段. 加载 可以使用dlopen在运行时加载一个so文件到当前进程的地址空间中. 这里需要注意dlopen的flag参数: RTLD_GLOBAL是必不可少的, 其含义是: 当前加载的so中的符号对其他so可见, 也就是说之后加载的so符号解析时, 可以使用这个so中的符号. 与其相对的是RTLD_LOCAL. 如果去掉RTLD_GLOBAL, 会出现以下这种情况: 注意表达式”f()”也会被”wrap”成一个函数, 并编译成so. RTLD_NOW也是必不可少的, 其含义是: 在加载so时就进行(函数)符号解析. 与之相对的是RTLD_LAZY, 表示对应的函数符号解析推迟到函数被调用时. 使用RTLD_LAZY参数有以下实验现象: 也就是说, 定义函数时可以使用当前还未定义的符号, dlopen时不会报错(因为不会进行符号解析). 但是调用时如果找不到对应的函数符号, 进程会直接crash, 结束运行. 而使用RTLD_NOW参数有以下实验现象: 也就是说定义函数时不允许使用当前还未定义的符号, 也不能直接使用未定义的函数计算, dlopen进行符号解析时就会出错. 因此, 这才是我们想要的行为.","categories":[{"name":"NJU OS Lab","slug":"NJU-OS-Lab","permalink":"https://ysln.github.io/categories/NJU-OS-Lab/"}],"tags":[{"name":"C","slug":"C","permalink":"https://ysln.github.io/tags/C/"},{"name":"REPL","slug":"REPL","permalink":"https://ysln.github.io/tags/REPL/"},{"name":"动态链接","slug":"动态链接","permalink":"https://ysln.github.io/tags/%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5/"}]}],"categories":[{"name":"NJU OS Lab","slug":"NJU-OS-Lab","permalink":"https://ysln.github.io/categories/NJU-OS-Lab/"}],"tags":[{"name":"C","slug":"C","permalink":"https://ysln.github.io/tags/C/"},{"name":"OS","slug":"OS","permalink":"https://ysln.github.io/tags/OS/"},{"name":"thread","slug":"thread","permalink":"https://ysln.github.io/tags/thread/"},{"name":"hart","slug":"hart","permalink":"https://ysln.github.io/tags/hart/"},{"name":"REPL","slug":"REPL","permalink":"https://ysln.github.io/tags/REPL/"},{"name":"动态链接","slug":"动态链接","permalink":"https://ysln.github.io/tags/%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5/"}]}